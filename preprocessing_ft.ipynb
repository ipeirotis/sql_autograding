{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ipeirotis/sql_autograding/blob/main/preprocessing_ft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd1yNAj1_e8H"
      },
      "outputs": [],
      "source": [
        "!pip install -q openai\n",
        "!pip install -q google-cloud-secret-manager\n",
        "!pip install -U PyMySQL sqlalchemy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gcsfs\n",
        "import pandas as pd\n",
        "import io\n",
        "import openai\n",
        "import os\n",
        "\n",
        "from google.cloud import secretmanager\n",
        "\n",
        "from google.colab import auth\n",
        "\n",
        "import requests\n",
        "\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "mvtWjL-WtlX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Login using the account that has access to the Google project\n",
        "# in order to access the resources for the project\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "tgSMZDnRr8wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def access_secret_version(project_id, secret_id, version_id):\n",
        "    \"\"\"\n",
        "    Access the payload of the given secret version and return it.\n",
        "\n",
        "    Args:\n",
        "        project_id (str): Google Cloud project ID.\n",
        "        secret_id (str): ID of the secret to access.\n",
        "        version_id (str): ID of the version to access.\n",
        "    Returns:\n",
        "        str: The secret version's payload, or None if\n",
        "        the version does not exist.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        client = secretmanager.SecretManagerServiceClient()\n",
        "        name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
        "        response = client.access_secret_version(request={\"name\": name})\n",
        "        return response.payload.data.decode('UTF-8')\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to access the secret version: {e}\")\n",
        "        return None\n",
        "\n",
        "openai_key = access_secret_version(\"sql-autograding\", \"openai-gpt4-32k\", \"3\")\n",
        "openai.api_key = openai_key\n"
      ],
      "metadata": {
        "id": "Jorn9T80BzgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"https://api.openai.com/v1/chat/completions\""
      ],
      "metadata": {
        "id": "3uASpahqoY2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grading = {\n",
        "    \"model\": \"gpt-4-32k\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": f\"question: There are two relationships between Customer and Plan. Explain how they differ. \\n Key: The Responsible For relationship is an overall 1:M relationship between Customer and Plan. A Customer can be responsible for 0, 1, or many Plans yet any one Plan will be linked to only 1 Customer for responsibility purposes. The Belongs relationship is an overall M:M relationship that permits the linking of multiple customers to a single plan, as in the case of family members being part of a particular plan or different plans. Student answer: Each customer can have 0 to many plans. Each plan must have one responsible party, but may belong to more than one customer. Grade on student answer based on the question and answer key.\"}],\n",
        "    \"max_tokens\": 256,\n",
        "    \"temperature\": 0\n",
        "}\n",
        "\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": f\"Bearer {openai_key}\"}\n",
        "\n",
        "response = requests.post(URL, headers=headers, json=grading, stream=False)\n",
        "if response.status_code != 200:\n",
        "    print(f\"Request to OpenAI failed with status {response.status_code}, response: {response.content}\")\n"
      ],
      "metadata": {
        "id": "r8pybyjT-2jL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.content)\n",
        "# print(response.choices.message.content)"
      ],
      "metadata": {
        "id": "awybK4_Bt5vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fs = gcsfs.GCSFileSystem(project='sql_autograding')\n",
        "with fs.open('gs://sql_autograding/quiz_responses.csv') as f:\n",
        "    data = pd.read_csv(f)\n"
      ],
      "metadata": {
        "id": "yNT0q9XaPf49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KY0-cyju8POl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "BeZzy_-68PXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove columns\n",
        "df = data.drop(columns=['QuestionAnswerTime'])\n",
        "df = df[df['QuestionType']=='Long Answer Question']\n",
        "df.head()"
      ],
      "metadata": {
        "id": "iI1XJpFlRbZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=1234)\n",
        "len(train_df), len(test_df)"
      ],
      "metadata": {
        "id": "akEYnaaBTdR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Each quizid is associated with a specific database. Music, Flights, Northwind, Facebook\n",
        "# It will need a bit of manual work to assign each quizid to a database,\n",
        "# but it will not take long (the questions are the same across quizzes offered for different sessions)."
      ],
      "metadata": {
        "id": "gSq72EKGA3wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create fine-tuning dataset for the model"
      ],
      "metadata": {
        "id": "cUTWawxSWgSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# assign each quiz with its database in df\n",
        "  # Module 4 Practice: Flights Database Questions - flights\n",
        "  # Restaurants Database: Simple Practice Queries - restaurants\n",
        "\n",
        "def map_db(quiz):\n",
        "  if 'Assignment 6' in quiz or 'Module 4 Practice: Flights' in quiz:\n",
        "    return 'flights'\n",
        "  elif 'Assignment 5' in quiz:\n",
        "    return 'imdb' #?\n",
        "  elif 'Assignment 4' in quiz or 'Assignment 3' in quiz or 'Assignment 2' in quiz:\n",
        "    return 'music'\n",
        "  elif 'Module 4' in quiz or 'Module 3' in quiz or 'Restaurants Database' in quiz:\n",
        "    return 'restaurants'\n",
        "  elif 'Module 2' in quiz:\n",
        "    return ['facebook', 'restaurants']\n",
        "  elif 'Module 1' in quiz:\n",
        "    return 'northwind'\n",
        "  elif 'General Data Analytics Practice' in quiz:\n",
        "    return 'collisions'\n",
        "  elif 'Final Exam' in quiz:\n",
        "    return ['northwind', 'flights']\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "df['Database'] = df['QuizName'].apply(map_db)"
      ],
      "metadata": {
        "id": "_OfGNMkeNjfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(20)"
      ],
      "metadata": {
        "id": "rHBs2RxdoFJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect to database"
      ],
      "metadata": {
        "id": "BeAItkj2qyuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "conn_string = 'mysql+pymysql://{user}:{password}@{host}/?charset=utf8'.format(\n",
        "    host = 'db.ipeirotis.org',\n",
        "    user = 'student',\n",
        "    password = 'dwdstudent2015',\n",
        "    encoding = 'utf-8')\n",
        "engine = create_engine(conn_string)"
      ],
      "metadata": {
        "id": "_r1BdoXeq68L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt and Message"
      ],
      "metadata": {
        "id": "4kVxqKvrrRW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt:\n",
        "In database XXX, we ask the question: “what is the average fare of the flights departing from each airport”?\n",
        "The student answered “SELECT …. FROM ….”\n",
        "The correct answer was “SELECT …. FROM ….”\n",
        "\n",
        "Message:\n",
        "The TA assigned the grade “8/10” points\n",
        "The feedback from the TA was “....”\n"
      ],
      "metadata": {
        "id": "O4Zy3dt7EmdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['prompt'] = \"In database \" + df['Database'].apply(lambda x: ' '.join(x)) + \", we ask the question:\\n\\n \" + df['Question'] + \"\\n\\n The studednt answered:\\n \" + df['InputUserAnswer'] + \"\\n\\n The correct answer was: \" + df['AnswerKey']\n",
        "df['message'] = \"The TA assigned the grade {s} points. The feedback from the TA was {feedback}\""
      ],
      "metadata": {
        "id": "0ogo3F8DWnYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "pidQbKuuviMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i, row in df.iterrows():\n",
        "#   db = row.Database\n",
        "#   q = row.Question\n",
        "#   ans = row.InputUserAnswer\n",
        "#   key = row.AnswerKey\n",
        "#   prompt = f'In database {db}, we ask the question: {q} The studednt answered {ans} The correct answer was {key}'\n",
        "\n",
        "#   s = row.Score\n",
        "#   feedback = row.CommentleftonUserResponse\n",
        "#   # how to get the full score\n",
        "#   message = f'The TA assigned the grade {s} points. The feedback from the TA was {feedback}'"
      ],
      "metadata": {
        "id": "UI76hBjaWnWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submit the dataset"
      ],
      "metadata": {
        "id": "ydOjG4UJWnpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = openai_key\n"
      ],
      "metadata": {
        "id": "j7On8FpZwHth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api fine_tunes.create -t test.jsonl -m ada --suffix \"grader_model\""
      ],
      "metadata": {
        "id": "eR7S33IxWtP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!openai tools fine_tunes.prepare_data -f grader_train.jsonl\n",
        "!openai tools fine_tunes.prepare_data -f grader_test.jsonl"
      ],
      "metadata": {
        "id": "kB8sGZUqWtYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api fine_tunes.create -t \"grader/grader_train.jsonl\" -v \"grader/grader_test.jsonl\" --batch_size 16"
      ],
      "metadata": {
        "id": "Wwqn2q4bgxEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use the fine-tuning model"
      ],
      "metadata": {
        "id": "g37x96oJWtv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grade a question"
      ],
      "metadata": {
        "id": "6fWgoAlfWwTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "MbYC8nYFD7bt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We compare the grade assigned by GPT to the grade assigned by the TA.\n",
        "# We will probably need to examine things critically when there are disagreements, as the difference may be also due to the TA being incorrect.\n"
      ],
      "metadata": {
        "id": "XhRCg8x1D6iM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}